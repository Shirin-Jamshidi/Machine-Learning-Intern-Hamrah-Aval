{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lqdt-kjAE-vo"
      },
      "source": [
        "#All"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7Nz9gMBH9Dce"
      },
      "source": [
        "#####Import library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbkuQLeD8sUz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from IPython.display import Image\n",
        "from PIL import Image\n",
        "from scipy.signal import correlate2d\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mHzTp1ps9OLF"
      },
      "source": [
        "#####Read file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vnljzYb9RlW",
        "outputId": "908a9a82-df78-4805-ec56-ddbbe3b4405b"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(r\"30.csv\")\n",
        "print(df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M-S8wpV599bt"
      },
      "source": [
        "#####find uniqe sub_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJzgB8yd_SCs",
        "outputId": "f89a3aad-56e1-4803-e150-79684d86ad28"
      },
      "outputs": [],
      "source": [
        "# Extract the unique IDs from the 'sub_id' column\n",
        "unique_ids = df['sub_id'].unique()\n",
        "\n",
        "# Print the unique IDs\n",
        "for unique_id in unique_ids:\n",
        "    print(unique_id)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Trjl7pMB_vRH"
      },
      "source": [
        "####how many uniqe id buy package and save data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B1aaWBYAMAKQ"
      },
      "source": [
        "#####count every user has how many pkg purchase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAZ_tIl0_u3M",
        "outputId": "632e2399-3f0d-4d10-8d5f-7f103a49355e"
      },
      "outputs": [],
      "source": [
        "# Count the occurrences of each unique ID\n",
        "id_counts = df['sub_id'].value_counts()\n",
        "\n",
        "# Print the ID counts\n",
        "for unique_id, count in id_counts.items():\n",
        "    print(f\"Unique ID: {unique_id}, Count: {count}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u1X1eWhKMXgh"
      },
      "source": [
        "##### plot data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "fqeXPm7-BP9D",
        "outputId": "5af6d8b1-9d91-4f52-a48a-bfb8afa2026f"
      },
      "outputs": [],
      "source": [
        "# Plotting the histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(id_counts, bins=100, alpha=0.75, color='blue')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Count of Unique IDs')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WfgFWCPFCo0x"
      },
      "source": [
        "##### create csv file for every unique id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl8fmrrNTKRd",
        "outputId": "0d5e6e19-682a-44ce-f9a9-91cd4900b4d6"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "folder_path = r'D:\\Daneshgah\\Az Shabake\\unique_id_behavior'\n",
        "\n",
        "# Use the shutil.rmtree() function to remove the entire folder and its contents\n",
        "shutil.rmtree(folder_path)\n",
        "\n",
        "#first delete\n",
        "# !rm -r 'unique_id_behavior'\n",
        "#then create folder\n",
        "# Create the folder if it doesn't exist\n",
        "folder_name = 'unique_id_behavior'\n",
        "if not os.path.exists(folder_name):\n",
        "    os.makedirs(folder_name)\n",
        "\n",
        "# Create a dictionary to store lists for each unique ID\n",
        "id_lists = {unique_id: [] for unique_id in id_counts.index}\n",
        "\n",
        "# Iterate over each row and append to the corresponding list\n",
        "for index, row in df.iterrows():\n",
        "    sub_id = row['sub_id']\n",
        "    id_lists[sub_id].append(row.tolist())\n",
        "\n",
        "minimum_data_needed =15\n",
        "# Save each list as a separate CSV file in the folder\n",
        "for unique_id, rows_list in id_lists.items():\n",
        "    if len(rows_list) >= minimum_data_needed:\n",
        "        # Create a DataFrame from the list of rows\n",
        "        sub_df = pd.DataFrame(rows_list, columns=df.columns)\n",
        "\n",
        "        # Sort the DataFrame based on the \"actvn_dt\" column\n",
        "        sub_df = sub_df.sort_values(by='actvn_dt')\n",
        "\n",
        "        # Add a new column \"times\" with incrementing values starting from 1\n",
        "        sub_df['times'] = range(1, len(sub_df) + 1)\n",
        "\n",
        "        # Save the DataFrame as a CSV file in the folder\n",
        "        file_path = os.path.join(folder_name, f'{unique_id}_30.csv')\n",
        "        sub_df.to_csv(file_path, index=False)\n",
        "        print(f'Saved rows for Unique ID {unique_id} in {file_path}')\n",
        "    else:\n",
        "        print(f'Skipped saving rows for Unique ID {unique_id} (less than {minimum_data_needed} rows)')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xQTJzeBjNRRg"
      },
      "source": [
        "##### make it zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYDrjsOeEYbn",
        "outputId": "ea87d614-89da-4ea2-eb21-acdbc819beaa"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "# Create a zip file of the folder\n",
        "zip_file_name = 'unique_id_behavior.zip'\n",
        "with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, _, files in os.walk(folder_name):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path, os.path.basename(file_path))\n",
        "\n",
        "print(f'Successfully created {zip_file_name}.')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eco1tyQyNZzw"
      },
      "source": [
        "#### train and predict last bought"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cDFPgh3Ax-Jh"
      },
      "source": [
        "#####predict next pkg linear reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from word2number import w2n\n",
        "import inflect\n",
        "\n",
        "def words_to_number(words):\n",
        "    try:\n",
        "        number = w2n.word_to_num(words)\n",
        "        return number\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def number_to_words(number):\n",
        "    p = inflect.engine()\n",
        "    return p.number_to_words(number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfgMG8nz6Zuo"
      },
      "outputs": [],
      "source": [
        "# Function to predict the next package using a decision tree classifier\n",
        "def predict_next_pkg_log_reg(X_train, y_train, next_time, last_pkg_bought):\n",
        "    y_train_word = []\n",
        "    for i in range(len(y_train)):\n",
        "       y_train_word.append(number_to_words(y_train[i]))\n",
        "\n",
        "    # Feature scaling using StandardScaler\n",
        "    sc_X = StandardScaler()\n",
        "    X_train_scaled = sc_X.fit_transform(X_train)\n",
        "\n",
        "    # # Encode the labels with OneHotEncoder\n",
        "    # encoder = OneHotEncoder(sparse=False)\n",
        "    # encoded_features = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "\n",
        "    # Fit the logistic reg classifier\n",
        "    lr_clf = LogisticRegression()\n",
        "    lr_clf.fit(X_train_scaled, y_train_word)\n",
        "\n",
        "    # Perform scaling on the next time data\n",
        "    next_time_scaled = sc_X.transform(np.array([next_time]))\n",
        "\n",
        "\n",
        "    # Predict the features\n",
        "    predicted_features = lr_clf.predict(next_time_scaled)\n",
        "    predicted_pkg =words_to_number(str(predicted_features[0]))\n",
        "\n",
        "    print(predicted_features)\n",
        "    # Inverse transform the one-hot encoded matrix to get the predicted label\n",
        "    if np.all(predicted_features == 0):\n",
        "        # Handle the case where all zeros are predicted\n",
        "        decoded_labels = \"No label predicted\"\n",
        "\n",
        "    return predicted_pkg\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk7GaOYiNyXf"
      },
      "source": [
        "#####Read files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O8XOzPwmrXa"
      },
      "outputs": [],
      "source": [
        "csv_files = [file for file in os.listdir(folder_name) if file.endswith('.csv')]\n",
        "real_data = []\n",
        "predict_next_pkg_data = []\n",
        "counter_noise = 0\n",
        "# Read each CSV file one by one\n",
        "columns_x = [2,3,4,5,6,7,10,17,25, 21,22,23,24]\n",
        "# 18,25\n",
        "for file in csv_files:\n",
        "    # Construct the file path\n",
        "    file_path = os.path.join(folder_name, file)\n",
        "\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Get information\n",
        "    X_train = df.iloc[:, columns_x].values\n",
        "    y_train = df[\"voice_pkg_code\"].values.reshape(-1, 1)\n",
        "    df[['prod_stat_eng', 'gender', 'Pk_ATL/BTL', 'device_type', 'Pk_regular/Event']] = df[['prod_stat_eng', 'gender', 'Pk_ATL/BTL', 'device_type', 'Pk_regular/Event']].apply(LabelEncoder().fit_transform)\n",
        "    X_train = X_train[:-1]\n",
        "    y_train = np.delete(y_train, -1)\n",
        "    sub_id = df[\"sub_id\"].iloc[-1]\n",
        "    next_time = df.iloc[-1, columns_x].values\n",
        "    last_pkg_bought = df[\"voice_pkg_code\"].iloc[-1]\n",
        "    \n",
        "    try:\n",
        "    # Predict\n",
        "        decoded_labels = predict_next_pkg_log_reg(X_train, y_train, next_time,last_pkg_bought)\n",
        "        print(decoded_labels)\n",
        "        predict_next_pkg_data.append(int(decoded_labels))\n",
        "        real_data.append(df[\"voice_pkg_code\"].iloc[-1])\n",
        "\n",
        "    except:\n",
        "        counter_noise = counter_noise + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkQ3_ISW7So5",
        "outputId": "3be170e0-8b8e-4d72-ccdb-abe86069074d"
      },
      "outputs": [],
      "source": [
        "print(\"accuracy_score = \",accuracy_score(real_data,predict_next_pkg_data))\n",
        "print(confusion_matrix(real_data, predict_next_pkg_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_features =[]\n",
        "accuracy_list = []\n",
        "\n",
        "n_features = X_train.shape[1]\n",
        "# print(n_features)\n",
        "selected_features = np.ones(n_features, dtype=bool)\n",
        "# print(selected_features)\n",
        "csv_files = [file for file in os.listdir(folder_name) if file.endswith('.csv')]\n",
        "initial_accuracy = 0.2\n",
        "for i in range(n_features - 1, 0, -1):\n",
        "    # Create a copy of selected_features, so we don't modify the original array\n",
        "    mask = np.copy(selected_features)\n",
        "    mask[i] = False\n",
        "    real_data = []\n",
        "    predict_next_pkg_data = []\n",
        "    counter_noise = 0\n",
        "    # Read each CSV file one by one\n",
        "    columns_x = [2,3,4,5,6,7,10,17,25, 21,22,23,24]\n",
        "    for file in csv_files:\n",
        "        # Construct the file path\n",
        "        file_path = os.path.join(folder_name, file)\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Get information\n",
        "        X_train = df.iloc[:, columns_x].values\n",
        "        next_time = df.iloc[-1, columns_x].values\n",
        "\n",
        "        # Select the subset of features based on the mask\n",
        "        X_train_subset = X_train[:, mask]\n",
        "        next_time_subset = next_time[mask]\n",
        "\n",
        "        y_train = df[\"voice_pkg_code\"].values.reshape(-1, 1)\n",
        "        X_train = X_train[:-1]\n",
        "        y_train = np.delete(y_train, -1)\n",
        "        sub_id = df[\"sub_id\"].iloc[-1]\n",
        "        last_pkg_bought = df[\"voice_pkg_code\"].iloc[-1]\n",
        "        # print(X_train)\n",
        "        try:\n",
        "        # Predict\n",
        "            decoded_labels = predict_next_pkg_log_reg(X_train_subset, y_train, next_time_subset,last_pkg_bought)\n",
        "            print(decoded_labels)\n",
        "            predict_next_pkg_data.append(int(decoded_labels))\n",
        "            real_data.append(df[\"voice_pkg_code\"].iloc[-1])\n",
        "\n",
        "        except:\n",
        "            counter_noise = counter_noise + 1\n",
        "    \n",
        "    # Evaluate the performance of the model on the test set\n",
        "    accuracy_subset = accuracy_score(real_data, predict_next_pkg_data)\n",
        "    list_features.append(np.where(mask)[0])\n",
        "    accuracy_list.append(accuracy_subset)\n",
        "    # If the accuracy improves, update the selected features\n",
        "    if accuracy_subset >= initial_accuracy:\n",
        "        selected_features = mask\n",
        "        initial_accuracy = accuracy_subset\n",
        "\n",
        "# Final selected features\n",
        "selected_features_indices = np.where(selected_features)[0]\n",
        "print(\"Final selected features indices:\", selected_features_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(list_features)\n",
        "print(accuracy_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "7UCMVgSZsqA4",
        "outputId": "b5049f02-b7aa-461f-e312-28cd5cf060be"
      },
      "outputs": [],
      "source": [
        "# Plot the histograms\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.hist(predict_next_pkg_data, bins=160, color='blue', alpha=0.5, label='predict')\n",
        "plt.hist(real_data, bins=160, color='red', alpha=0.5, label='last bought')\n",
        "\n",
        "plt.xlabel('voice_pkg_code')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('predict with decision tree and ')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
