{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lqdt-kjAE-vo"
      },
      "source": [
        "# All"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7Nz9gMBH9Dce"
      },
      "source": [
        "##### Import library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbkuQLeD8sUz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "from scipy.signal import correlate2d\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mHzTp1ps9OLF"
      },
      "source": [
        "##### Read file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vnljzYb9RlW",
        "outputId": "908a9a82-df78-4805-ec56-ddbbe3b4405b"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(r\"1 hour.csv\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"usage_dur_V2\"] = df[\"usage_dur\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(df[\"usage_dur_V2\"])):\n",
        "    if df[\"usage_dur_V2\"][i] == 30:\n",
        "        df[\"usage_dur_V2\"][i] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"avg_usage\"] = df[\"usage_dur\"]\n",
        "for i in range(len(df[\"avg_usage\"])):\n",
        "    if df[\"usage_dur\"][i]!= 0 :\n",
        "        df[\"avg_usage\"][i] =df[\"voi_pkg_acc_usg_dur\"][i] / df[\"usage_dur\"][i]\n",
        "    else:\n",
        "        df[\"avg_usage\"][i]=df[\"voi_pkg_acc_usg_dur\"][i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"voi_pkg_rmng_dur_neg\"] = df[\"voi_pkg_rmng_dur\"]\n",
        "for i in range(len(df[\"voi_pkg_rmng_dur\"])):\n",
        "    df[\"voi_pkg_rmng_dur_neg\"][i] = -1*df[\"voi_pkg_rmng_dur_neg\"][i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(r'H:\\GIT project\\Hamrah Aval\\1h_new.csv', index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M-S8wpV599bt"
      },
      "source": [
        "##### find uniqe sub_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJzgB8yd_SCs",
        "outputId": "f89a3aad-56e1-4803-e150-79684d86ad28"
      },
      "outputs": [],
      "source": [
        "# Extract the unique IDs from the 'sub_id' column\n",
        "unique_ids = df['sub_id'].unique()\n",
        "\n",
        "# Print the unique IDs\n",
        "# for unique_id in unique_ids:\n",
        "#     print(unique_id)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Trjl7pMB_vRH"
      },
      "source": [
        "####how many uniqe id buy package and save data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B1aaWBYAMAKQ"
      },
      "source": [
        "#####count every user has how many pkg purchase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAZ_tIl0_u3M",
        "outputId": "632e2399-3f0d-4d10-8d5f-7f103a49355e"
      },
      "outputs": [],
      "source": [
        "# Count the occurrences of each unique ID\n",
        "id_counts = df['sub_id'].value_counts()\n",
        "\n",
        "# Print the ID counts\n",
        "for unique_id, count in id_counts.items():\n",
        "    print(f\"Unique ID: {unique_id}, Count: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(id_counts)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u1X1eWhKMXgh"
      },
      "source": [
        "##### plot data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "fqeXPm7-BP9D",
        "outputId": "5af6d8b1-9d91-4f52-a48a-bfb8afa2026f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "# Plotting the histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(id_counts, bins=100, alpha=0.75, color='blue')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Count of Unique IDs')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WfgFWCPFCo0x"
      },
      "source": [
        "##### create csv file for every unique id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl8fmrrNTKRd",
        "outputId": "0d5e6e19-682a-44ce-f9a9-91cd4900b4d6"
      },
      "outputs": [],
      "source": [
        "minimum_data_needed = 16\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "folder_name = 'unique_id_behavior'\n",
        "\n",
        "#first delete\n",
        "counterrr = 0\n",
        "\n",
        "folder_path = r'H:\\GIT project\\Hamrah Aval\\unique_id_behavior'\n",
        "df = pd.read_csv(r\"1h_new.csv\")\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(folder_path)\n",
        "    print(f\"Folder '{folder_path}' has been deleted successfully.\")\n",
        "except OSError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "#then create folder\n",
        "# Create the folder if it doesn't exist\n",
        "folder_name = 'unique_id_behavior'\n",
        "if not os.path.exists(folder_name):\n",
        "    os.makedirs(folder_name)\n",
        "id_lists = {}\n",
        "print(\"deleted \")\n",
        "# Create a dictionary to store lists for each unique ID\n",
        "id_lists = {unique_id: [] for unique_id in id_counts.index}\n",
        "\n",
        "# Iterate over each row and append to the corresponding list\n",
        "for index, row in df.iterrows():\n",
        "    sub_id = row['sub_id']\n",
        "    id_lists[sub_id].append(row.tolist())\n",
        "# Save each list as a separate CSV file in the folder\n",
        "for unique_id, rows_list in id_lists.items():\n",
        "    if len(rows_list) >= minimum_data_needed:\n",
        "        # Create a DataFrame from the list of rows\n",
        "        sub_df = pd.DataFrame(rows_list, columns=df.columns)\n",
        "\n",
        "        # Sort the DataFrame based on the \"actvn_dt\" column\n",
        "        sub_df = sub_df.sort_values(by='actvn_dt')\n",
        "\n",
        "        # Add a new column \"times\" with incrementing values starting from 1\n",
        "        sub_df['times'] = range(1, len(sub_df) + 1)\n",
        "\n",
        "        # Save the DataFrame as a CSV file in the folder\n",
        "        file_path = os.path.join(folder_name, f'{unique_id}_1.csv')\n",
        "        sub_df.to_csv(file_path, index=False)\n",
        "        counterrr = 1 +counterrr\n",
        "        # print(f'Saved rows for Unique ID {unique_id} in {file_path}')\n",
        "    else:\n",
        "        print(f'Skipped saving rows for Unique ID {unique_id} (less than {minimum_data_needed} rows)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(counterrr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def range5to20(minimum_data_needed):\n",
        "        df = pd.read_csv(r\"1h_new.csv\")\n",
        "\n",
        "        try:\n",
        "            shutil.rmtree(folder_path)\n",
        "            print(f\"Folder '{folder_path}' has been deleted successfully.\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error: {e}\")\n",
        "        #then create folder\n",
        "        # Create the folder if it doesn't exist\n",
        "        folder_name = 'unique_id_behavior'\n",
        "        if not os.path.exists(folder_name):\n",
        "            os.makedirs(folder_name)\n",
        "\n",
        "        # Create a dictionary to store lists for each unique ID\n",
        "        id_lists = {unique_id: [] for unique_id in id_counts.index}\n",
        "\n",
        "        # Iterate over each row and append to the corresponding list\n",
        "        for index, row in df.iterrows():\n",
        "            sub_id = row['sub_id']\n",
        "            id_lists[sub_id].append(row.tolist())\n",
        "\n",
        "        # minimum_data_needed = 5\n",
        "        # Save each list as a separate CSV file in the folder\n",
        "        for unique_id, rows_list in id_lists.items():\n",
        "            if len(rows_list) >= minimum_data_needed:\n",
        "                # Create a DataFrame from the list of rows\n",
        "                sub_df = pd.DataFrame(rows_list, columns=df.columns)\n",
        "\n",
        "                # Sort the DataFrame based on the \"actvn_dt\" column\n",
        "                sub_df = sub_df.sort_values(by='actvn_dt')\n",
        "\n",
        "                # Add a new column \"times\" with incrementing values starting from 1\n",
        "                sub_df['times'] = range(1, len(sub_df) + 1)\n",
        "\n",
        "                # Save the DataFrame as a CSV file in the folder\n",
        "                file_path = os.path.join(folder_name, f'{unique_id}_1.csv')\n",
        "                sub_df.to_csv(file_path, index=False)\n",
        "                # print(f'Saved rows for Unique ID {unique_id} in {file_path}')\n",
        "            else:\n",
        "                print(f'Skipped saving rows for Unique ID {unique_id} (less than {minimum_data_needed} rows)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def range5to20_only_value(minimum_data_needed):\n",
        "        df = pd.read_csv(r\"1h_new.csv\")\n",
        "\n",
        "        try:\n",
        "            shutil.rmtree(folder_path)\n",
        "            print(f\"Folder '{folder_path}' has been deleted successfully.\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error: {e}\")\n",
        "        #then create folder\n",
        "        # Create the folder if it doesn't exist\n",
        "        folder_name = 'unique_id_behavior'\n",
        "        if not os.path.exists(folder_name):\n",
        "            os.makedirs(folder_name)\n",
        "\n",
        "        print(\"deleted \")\n",
        "        # Create a dictionary to store lists for each unique ID\n",
        "        id_lists = {unique_id: [] for unique_id in id_counts.index}\n",
        "\n",
        "        # Iterate over each row and append to the corresponding list\n",
        "        for index, row in df.iterrows():\n",
        "            sub_id = row['sub_id']\n",
        "            id_lists[sub_id].append(row.tolist())\n",
        "\n",
        "        # minimum_data_needed = 5\n",
        "        # Save each list as a separate CSV file in the folder\n",
        "        for unique_id, rows_list in id_lists.items():\n",
        "            if len(rows_list) == minimum_data_needed:\n",
        "                # Create a DataFrame from the list of rows\n",
        "                sub_df = pd.DataFrame(rows_list, columns=df.columns)\n",
        "\n",
        "                # Sort the DataFrame based on the \"actvn_dt\" column\n",
        "                sub_df = sub_df.sort_values(by='actvn_dt')\n",
        "\n",
        "                # Add a new column \"times\" with incrementing values starting from 1\n",
        "                sub_df['times'] = range(1, len(sub_df) + 1)\n",
        "\n",
        "                # Save the DataFrame as a CSV file in the folder\n",
        "                file_path = os.path.join(folder_name, f'{unique_id}_1.csv')\n",
        "                sub_df.to_csv(file_path, index=False)\n",
        "                # print(f'Saved rows for Unique ID {unique_id} in {file_path}')\n",
        "            else:\n",
        "                print(f'Skipped saving rows for Unique ID {unique_id} (less than {minimum_data_needed} rows)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_specific_column(file_path, column_name):\n",
        "    # Read the CSV file into a pandas DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Get the last row of the DataFrame\n",
        "    last_row = df.iloc[-1]\n",
        "\n",
        "    # Get the value of the specific column from the last row\n",
        "    last_row_column_value = last_row[column_name]\n",
        "    other = df[column_name]\n",
        "    other = other.drop(df.index[-1])\n",
        "    # Check if the specific column value is not similar to any of the last row values\n",
        "    if not any(other.isin([last_row_column_value])):\n",
        "        # Delete the CSV file if the condition is met\n",
        "        os.remove(file_path)\n",
        "        print(f\"File '{file_path}' has been deleted.\")\n",
        "\n",
        "\n",
        "directory_path =  r\"H:\\GIT project\\Hamrah Aval\\unique_id_behavior\"\n",
        "specific_column_name = 'voice_pkg_code'   # The name of the specific column to compare\n",
        "\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith('.csv'):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        compare_specific_column(file_path, specific_column_name)\n",
        "\n",
        "\n",
        "def check_file_last_item():\n",
        "# Loop through all CSV files in the directory\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith('.csv'):\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            compare_specific_column(file_path, specific_column_name)           "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# def count_files_in_folder(folder_path):\n",
        "#     try:\n",
        "#         # Get a list of all files in the folder\n",
        "#         files = os.listdir(folder_path)\n",
        "#         # Count the number of files in the list\n",
        "#         num_files = len(files)\n",
        "#         return num_files\n",
        "#     except FileNotFoundError:\n",
        "#         print(f\"Folder not found: {folder_path}\")\n",
        "#         return None\n",
        "\n",
        "\n",
        "# num_of_files = []\n",
        "# least_pkg_bought = []\n",
        "# for k in range(5,21):\n",
        "#    range5to20(k)\n",
        "#    check_file_last_item()\n",
        "#    least_pkg_bought.append(k)\n",
        "#    num_of_files.append(count_files_in_folder(r'H:\\GIT project\\Hamrah Aval\\unique_id_behavior'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# # plt.plot(least_num,accurucy_ll)\n",
        "# plt.plot(least_pkg_bought, num_of_files, color='blue', marker='o', linestyle='-', linewidth=2)\n",
        "# plt.xlabel('num of pkg bought')\n",
        "# plt.ylabel('num of users')\n",
        "# plt.title('number of  pkg atleast bought')\n",
        "# # plt.title('number of specific num of pkg bought')\n",
        "# plt.grid(True)  # Add a grid to the plot\n",
        "# plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xQTJzeBjNRRg"
      },
      "source": [
        "##### make it zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYDrjsOeEYbn",
        "outputId": "ea87d614-89da-4ea2-eb21-acdbc819beaa"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "# Create a zip file of the folder\n",
        "zip_file_name = 'unique_id_behavior.zip'\n",
        "with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, _, files in os.walk(folder_name):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path, os.path.basename(file_path))\n",
        "\n",
        "print(f'Successfully created {zip_file_name}.')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eco1tyQyNZzw"
      },
      "source": [
        "#### train and predict last bought"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### nom to word and inverse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from word2number import w2n\n",
        "import inflect\n",
        "\n",
        "def words_to_number(words):\n",
        "    try:\n",
        "        number = w2n.word_to_num(words)\n",
        "        return number\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def number_to_words(number):\n",
        "    p = inflect.engine()\n",
        "    return p.number_to_words(number)\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cDFPgh3Ax-Jh"
      },
      "source": [
        "##### predict next pkg linear reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfgMG8nz6Zuo"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "predict_nex_pkg_data_list = []\n",
        "def predict_next_pkg_linear(X_train, y_train, next_time):\n",
        "    # Feature scaling using StandardScaler\n",
        "    sc_X = StandardScaler()\n",
        "    X_train_scaled = sc_X.fit_transform(X_train)\n",
        "\n",
        "    # Encode the labels with OneHotEncoder\n",
        "    encoder = OneHotEncoder(sparse=False)\n",
        "    encoded_features = encoder.fit_transform(y_train.reshape(-1,1))\n",
        "    # print(X_train_scaled)\n",
        "    # Fit Linear Regression model\n",
        "    lin_reg = LinearRegression()\n",
        "    lin_reg.fit(X_train_scaled, encoded_features)\n",
        "\n",
        "    # Perform decoding\n",
        "    next_time_scaled = sc_X.transform(np.array([next_time]))\n",
        "    predicted_features = lin_reg.predict(next_time_scaled)\n",
        "    \n",
        "    # Inverse transform the one-hot encoded matrix to get the predicted label\n",
        "    decoded_labels = encoder.inverse_transform(predicted_features)\n",
        "\n",
        "    return decoded_labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "all_columns = [4,7, 21,22,23,28]\n",
        "\n",
        "# Get a list of all CSV files in the folder\n",
        "csv_files = [file for file in os.listdir(folder_name) if file.endswith('.csv')]\n",
        "real_data = []\n",
        "predict_nex_pkg_data = []\n",
        "y_train_word = []\n",
        "# Read each CSV file one by one\n",
        "for file in csv_files:\n",
        "    # Construct the file path\n",
        "    file_path = os.path.join(folder_name, file)\n",
        "\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Get information\n",
        "    # X_train = np.ravel(np.array(df[\"times\"].values).reshape(-1, 1))\n",
        "    # y_train = np.ravel(np.array(df[\"voice_pkg_code\"].values).reshape(-1, 1))\n",
        "    \n",
        "\n",
        "    X_train = df.iloc[:, all_columns].values\n",
        "    y_train = df[\"voice_pkg_code\"].values\n",
        "    sub_id = df[\"sub_id\"].iloc[-1]\n",
        "    # next_time = df[\"times\"].iloc[-1] + 1\n",
        "    last_pkg_bought = df[\"voice_pkg_code\"].iloc[-1]\n",
        "    real_data.append( df[\"voice_pkg_code\"].iloc[-1])\n",
        "    next_time = df.iloc[-1, all_columns].values\n",
        "\n",
        "    # Predict\n",
        "    decoded_labels = predict_next_pkg_linear(X_train,y_train , next_time)\n",
        "    # if(last_pkg_bought != int(decoded_labels[0][0])):\n",
        "    #   print(f'{sub_id}  last pkg bought : {last_pkg_bought}  next pkg predicted {decoded_labels}.')\n",
        "    predict_nex_pkg_data_list.append(decoded_labels[0][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"accuracy_score = \",accuracy_score(real_data,predict_nex_pkg_data_list))\n",
        "print(confusion_matrix(real_data, predict_nex_pkg_data_list))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### predict next pkg SVM reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn import svm\n",
        "import os\n",
        "\n",
        "\n",
        "folder_name = 'unique_id_behavior'\n",
        "\n",
        "predict_nex_pkg_data_list_svm = []\n",
        "# Function to predict the next package using Linear Regression\n",
        "def predict_next_pkg_svm(X_train, y_train, next_time):\n",
        "    # Feature scaling using StandardScaler\n",
        "    sc_X = StandardScaler()\n",
        "    X_train_scaled = sc_X.fit_transform(X_train)\n",
        "    # print(y_train)\n",
        "\n",
        "    unique_values_count = len(set(y_train))\n",
        "    unique_values_count = len({x: True for x in y_train})\n",
        "\n",
        "    if(unique_values_count != 1):\n",
        "        classifier = svm.SVC( kernel = 'linear', decision_function_shape='ovr')  # You can choose different kernels like 'linear', 'rbf', 'poly', etc.\n",
        "        # Train the model\n",
        "        classifier.fit(X_train_scaled, y_train)\n",
        "        # Perform decoding\n",
        "        next_time_scaled = sc_X.transform(np.array([next_time]))\n",
        "        predicted_features = classifier.predict(next_time_scaled)\n",
        "        predicted_pkg =words_to_number(str(predicted_features[0]))\n",
        "        return predicted_pkg\n",
        "    else:\n",
        "        return words_to_number(y_train[-1])\n",
        "\n",
        "csv_files = [file for file in os.listdir(folder_name) if file.endswith('.csv')]\n",
        "real_data = []\n",
        "predict_nex_pkg_data = []\n",
        "all_columns = [4, 7, 23, 28]\n",
        "# Read each CSV file one by one\n",
        "for file in csv_files:\n",
        "    # Construct the file path\n",
        "    file_path = os.path.join(folder_name, file)\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "    y_train_word = []\n",
        "\n",
        "\n",
        "    X_train = df.iloc[:, all_columns].values\n",
        "    y_train = df[\"voice_pkg_code\"].values\n",
        "    sub_id = df[\"sub_id\"].iloc[-1]\n",
        "    last_pkg_bought = df[\"voice_pkg_code\"].iloc[-1]\n",
        "    next_time = df.iloc[-1, all_columns].values\n",
        "    real_data.append(df[\"voice_pkg_code\"].iloc[-1])\n",
        "    for i in range(len(y_train)):\n",
        "        y_train_word.append(number_to_words(y_train[i]))\n",
        "    # print(\"jo\")\n",
        "    # Predict\n",
        "    pred_pkg = predict_next_pkg_svm(X_train, y_train_word , next_time)\n",
        "    predict_nex_pkg_data_list_svm.append(pred_pkg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"accuracy_score = \",accuracy_score(real_data,predict_nex_pkg_data_list_svm))\n",
        "print(confusion_matrix(real_data, predict_nex_pkg_data_list_svm))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### predict next with naive_bayes "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "# Function to predict the next package using Linear Regression\n",
        "def predict_next_pkg_naive_bayess(X_train, y_train, next_time):\n",
        "    # Feature scaling using StandardScaler\n",
        "    sc_X = StandardScaler()\n",
        "    X_train_scaled = sc_X.fit_transform(X_train)\n",
        "    # print(y_train)\n",
        "\n",
        "    unique_values_count = len(set(y_train))\n",
        "    unique_values_count = len({x: True for x in y_train})\n",
        "    if(unique_values_count != 1):\n",
        "        classifier = GaussianNB()\n",
        "        # Train the model\n",
        "        classifier.fit(X_train_scaled, y_train)\n",
        "        # Perform decoding\n",
        "        next_time_scaled = sc_X.transform(np.array([next_time]))\n",
        "        predicted_features = classifier.predict(next_time_scaled)\n",
        "        predicted_pkg =words_to_number(str(predicted_features[0]))\n",
        "        \n",
        "        return predicted_pkg\n",
        "    else:\n",
        "        return words_to_number(y_train[-1])\n",
        "    # return decoded_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"accuracy_score = \",accuracy_score(real_data, predict_next_pkg_data))\n",
        "print(confusion_matrix(real_data, predict_next_pkg_data))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def predict_next_pkg_log_reg(X_train, y_train, next_time, last_pkg_bought):\n",
        "    y_train_word = []\n",
        "        \n",
        "    # Convert the list to a set to get unique elements\n",
        "    # Find the number of unique words\n",
        "    num_unique_words = len(set(y_train))\n",
        "    if num_unique_words == 1 :\n",
        "        return y_train[1]\n",
        "    for i in range(len(y_train)):\n",
        "       y_train_word.append(number_to_words(y_train[i]))\n",
        "\n",
        "    # Feature scaling using StandardScaler\n",
        "    sc_X = StandardScaler()\n",
        "    X_train_scaled = sc_X.fit_transform(X_train)\n",
        "\n",
        "    # # Encode the labels with OneHotEncoder\n",
        "    # encoder = OneHotEncoder(sparse=False)\n",
        "    # encoded_features = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "\n",
        "    # Fit the logistic reg classifier\n",
        "    lr_clf = LogisticRegression()\n",
        "    lr_clf.fit(X_train_scaled, y_train_word)\n",
        "\n",
        "    # Perform scaling on the next time data\n",
        "    next_time_scaled = sc_X.transform(np.array([next_time]))\n",
        "\n",
        "\n",
        "    # Predict the features\n",
        "    predicted_features = lr_clf.predict(next_time_scaled)\n",
        "    predicted_pkg =words_to_number(str(predicted_features[0]))\n",
        "\n",
        "    print(predicted_features)\n",
        "    # Inverse transform the one-hot encoded matrix to get the predicted label\n",
        "    if np.all(predicted_features == 0):\n",
        "        # Handle the case where all zeros are predicted\n",
        "        decoded_labels = \"No label predicted\"\n",
        "\n",
        "    return predicted_pkg\n",
        "\n",
        "\n",
        "csv_files = [file for file in os.listdir(folder_name) if file.endswith('.csv')]\n",
        "real_data = []\n",
        "predict_next_pkg_data = []\n",
        "counter_noise = 0\n",
        "# Read each CSV file one by one\n",
        "columns_x = [10, 2,3,4,6,7,25, 21,22,23,24]\n",
        "\n",
        "# 18,25\n",
        "for file in csv_files:\n",
        "    # Construct the file path\n",
        "    file_path = os.path.join(folder_name, file)\n",
        "\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Get information\n",
        "    X_train = df.iloc[:, columns_x].values\n",
        "    y_train = df[\"voice_pkg_code\"].values.reshape(-1, 1)\n",
        "    df[['prod_stat_eng', 'gender', 'Pk_ATL/BTL', 'device_type', 'Pk_regular/Event']] = df[['prod_stat_eng', 'gender', 'Pk_ATL/BTL', 'device_type', 'Pk_regular/Event']].apply(LabelEncoder().fit_transform)\n",
        "    X_train = X_train[:-1]\n",
        "    y_train = np.delete(y_train, -1)\n",
        "    sub_id = df[\"sub_id\"].iloc[-1]\n",
        "    next_time = df.iloc[-1, columns_x].values\n",
        "    last_pkg_bought = df[\"voice_pkg_code\"].iloc[-1]\n",
        "    # for i in range(len(y_train)):\n",
        "    #     y_train_word.append(number_to_words(y_train[i]))\n",
        "    # Predict\n",
        "    decoded_labels = predict_next_pkg_log_reg(X_train, y_train, next_time, last_pkg_bought)\n",
        "    predict_next_pkg_data.append(int(decoded_labels))\n",
        "    real_data.append(df[\"voice_pkg_code\"].iloc[-1])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"accuracy_score = \",accuracy_score(real_data, predict_next_pkg_data))\n",
        "print(confusion_matrix(real_data, predict_next_pkg_data))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk7GaOYiNyXf"
      },
      "source": [
        "##### Read files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O8XOzPwmrXa"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score,precision_score, recall_score\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "real_data = []\n",
        "predict_nex_pkg_data_linear = []\n",
        "predict_nex_pkg_data_svm = []\n",
        "predict_nex_pkg_data_naive_bayes = []\n",
        "accurucy_ll = []\n",
        "least_num = []\n",
        "# Read each CSV file one by one\n",
        "# ,6\n",
        "columns_x = [4,7, 23,28]\n",
        "\n",
        "for k in range(5,21):\n",
        "   real_data = []\n",
        "   predict_nex_pkg_data_linear = []\n",
        "   predict_nex_pkg_data_svm = []\n",
        "\n",
        "   range5to20(k)\n",
        "   check_file_last_item()\n",
        "\n",
        "   folder_name = 'unique_id_behavior'\n",
        "\n",
        "   csv_files = [file for file in os.listdir(folder_name) if file.endswith('.csv')]\n",
        "   for file in csv_files:\n",
        "          # Construct the file path\n",
        "          file_path = os.path.join(folder_name, file)\n",
        "\n",
        "          # Read the CSV file\n",
        "          df = pd.read_csv(file_path)\n",
        "\n",
        "          # Get information\n",
        "          X_train = df.iloc[:, all_columns].values\n",
        "          y_train = df[\"voice_pkg_code\"].values\n",
        "          sub_id = df[\"sub_id\"].iloc[-1]\n",
        "          last_pkg_bought = df[\"voice_pkg_code\"].iloc[-1]\n",
        "          next_time = df.iloc[-1, all_columns].values\n",
        "          \n",
        "          y_train_word = []\n",
        "\n",
        "          for i in range(len(y_train)):\n",
        "               y_train_word.append(number_to_words(y_train[i]))\n",
        "          real_data.append( df[\"voice_pkg_code\"].iloc[-1])\n",
        "          \n",
        "          # Predict\n",
        "          y_pred_svm = predict_next_pkg_svm(X_train,y_train_word , next_time)\n",
        "          predict_nex_pkg_data_svm.append(y_pred_svm)\n",
        "   accurucy_ll.append(accuracy_score(real_data,predict_nex_pkg_data_svm))\n",
        "   least_num.append(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# plt.plot(least_num,accurucy_ll)\n",
        "plt.plot(least_num, accurucy_ll, color='blue', marker='o', linestyle='-', linewidth=2)\n",
        "plt.xlabel('num of pkg bought')\n",
        "plt.ylabel('accuracy score ')\n",
        "# plt.title('accuracy of specific num of pkg bought')\n",
        "plt.title('accuracy of least pkg bought')\n",
        "plt.grid(True)  # Add a grid to the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score,precision_score, recall_score\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "real_data = []\n",
        "predict_nex_pkg_data_linear = []\n",
        "predict_nex_pkg_data_svm = []\n",
        "predict_nex_pkg_data_naive_bayes = []\n",
        "accurucy_ll = []\n",
        "least_num = []\n",
        "# Read each CSV file one by one\n",
        "all_columns = [4, 7, 23, 28]\n",
        "\n",
        "for k in range(5,20):\n",
        "   real_data = []\n",
        "   predict_nex_pkg_data_linear = []\n",
        "   predict_nex_pkg_data_svm = []\n",
        "\n",
        "   range5to20_only_value(k)\n",
        "   check_file_last_item()\n",
        "\n",
        "   folder_name = 'unique_id_behavior'\n",
        "\n",
        "   csv_files = [file for file in os.listdir(folder_name) if file.endswith('.csv')]\n",
        "   for file in csv_files:\n",
        "          # Construct the file path\n",
        "          file_path = os.path.join(folder_name, file)\n",
        "\n",
        "          # Read the CSV file\n",
        "          df = pd.read_csv(file_path)\n",
        "\n",
        "          # Get information\n",
        "          X_train = df.iloc[:, all_columns].values\n",
        "          y_train = df[\"voice_pkg_code\"].values\n",
        "          sub_id = df[\"sub_id\"].iloc[-1]\n",
        "          last_pkg_bought = df[\"voice_pkg_code\"].iloc[-1]\n",
        "          next_time = df.iloc[-1, all_columns].values\n",
        "          y_train_word = []\n",
        "\n",
        "          for i in range(len(y_train)):\n",
        "               y_train_word.append(number_to_words(y_train[i]))\n",
        "          \n",
        "\n",
        "          # Predict\n",
        "          y_pred_svm = predict_next_pkg_svm(X_train,y_train_word , next_time)\n",
        "          predict_nex_pkg_data_svm.append(y_pred_svm)\n",
        "          real_data.append( df[\"voice_pkg_code\"].iloc[-1])\n",
        "   accurucy_ll.append(accuracy_score(real_data,predict_nex_pkg_data_svm))\n",
        "   least_num.append(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# plt.plot(least_num,accurucy_ll)\n",
        "plt.plot(least_num, accurucy_ll, color='blue', marker='o', linestyle='-', linewidth=2)\n",
        "plt.xlabel('num of pkg bought')\n",
        "plt.ylabel('accuracy score ')\n",
        "plt.title('accuracy of specific num of pkg bought')\n",
        "plt.grid(True)  # Add a grid to the plot\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkQ3_ISW7So5",
        "outputId": "3be170e0-8b8e-4d72-ccdb-abe86069074d"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score,precision_score, recall_score\n",
        "print(\"accuracy_score = \",accuracy_score(real_data,predict_nex_pkg_data))\n",
        "print(confusion_matrix(real_data, predict_nex_pkg_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "list_features =[]\n",
        "accuracy_list = []\n",
        "folder_name = 'unique_id_behavior'\n",
        "predict_nex_pkg_data_list = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def predict_next_pkg_linear(X_train, y_train, next_time):\n",
        "    # Feature scaling using StandardScaler\n",
        "    sc_X = StandardScaler()\n",
        "    X_train_scaled = sc_X.fit_transform(X_train)\n",
        "\n",
        "    # Encode the labels with OneHotEncoder\n",
        "    encoder = OneHotEncoder(sparse=False)\n",
        "    encoded_features = encoder.fit_transform(y_train.reshape(-1,1))\n",
        "    # print(X_train_scaled)\n",
        "    # Fit Linear Regression model\n",
        "    lin_reg = LinearRegression()\n",
        "    lin_reg.fit(X_train_scaled, encoded_features)\n",
        "\n",
        "    # Perform decoding\n",
        "    next_time_scaled = sc_X.transform(np.array([next_time]))\n",
        "    predicted_features = lin_reg.predict(next_time_scaled)\n",
        "    \n",
        "    # Inverse transform the one-hot encoded matrix to get the predicted label\n",
        "    decoded_labels = encoder.inverse_transform(predicted_features)\n",
        "\n",
        "    return decoded_labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "csv_files = [file for file in os.listdir(folder_name) if file.endswith('.csv')]\n",
        "initial_accuracy = 0.2\n",
        "# ,,5,10,17,6 , 22,23,3,28,21\n",
        "all_columns = [4,7, 21,28]\n",
        "\n",
        "#[25, 7, 2, 26, 24, 27]Final selected features indices: [7, 2, 26, 24, 27, 21]\n",
        "#[2, 3, 6, 24, 25, 23]22 23\n",
        "\n",
        "\n",
        "selected_features = []\n",
        "\n",
        "\n",
        "for i in range(len(all_columns)):\n",
        "    # Create a copy of selected_features, so we don't modify the original array\n",
        "    columns_copy = all_columns.copy()\n",
        "    \n",
        "    # Remove the element at index i from the copy\n",
        "    del columns_copy[i]\n",
        "\n",
        "    real_data = []\n",
        "    predict_next_pkg_data = []\n",
        "    counter_noise = 0\n",
        "    \n",
        "    # Read each CSV file one by one\n",
        "\n",
        "    for file in csv_files:\n",
        "        # Construct the file path\n",
        "        file_path = os.path.join(folder_name, file)\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "\n",
        "        #Get information\n",
        "        X_train = df.iloc[:, columns_copy].values\n",
        "        y_train = df[\"voice_pkg_code\"].values\n",
        "        sub_id = df[\"sub_id\"].iloc[-1]\n",
        "        last_pkg_bought = df[\"voice_pkg_code\"].iloc[-1]\n",
        "        next_time = df.iloc[-1, columns_copy].values\n",
        "        \n",
        "        decoded_labels = predict_next_pkg_linear(X_train, y_train, next_time)\n",
        "        predict_next_pkg_data.append(int(decoded_labels[0][0]))\n",
        "        real_data.append(df[\"voice_pkg_code\"].iloc[-1])\n",
        "\n",
        "    # Evaluate the performance of the model on the test set\n",
        "    accuracy_subset = accuracy_score(real_data, predict_next_pkg_data)\n",
        "    list_features.append(columns_copy)\n",
        "    accuracy_list.append(accuracy_subset)\n",
        "    # If the accuracy improves, update the selected features\n",
        "    if accuracy_subset >= initial_accuracy:\n",
        "        selected_features = columns_copy\n",
        "        initial_accuracy = accuracy_subset\n",
        "\n",
        "# Final selected features\n",
        "print(\"Final selected features indices:\", selected_features)\n",
        "#[7, 3, 4] 721\n",
        "# [25, 7, 2, 26, 24, 27] 0.675\n",
        "# [25, 7, 2, 26, 24, 27] 0.676056338028169"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(selected_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(list_features)\n",
        "print(accuracy_list)\n",
        "\n",
        "# [[7, 21, 23, 28], [4, 21, 23, 28], [4, 7, 23, 28], [4, 7, 21, 28], [4, 7, 21, 23]]\n",
        "# [0.696319018404908, 0.7668711656441718, 0.7668711656441718, 0.7668711656441718, 0.7070552147239264]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "7UCMVgSZsqA4",
        "outputId": "b5049f02-b7aa-461f-e312-28cd5cf060be"
      },
      "outputs": [],
      "source": [
        "# Plot the histograms\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.hist(predict_nex_pkg_data, bins=160, color='blue', alpha=0.5, label='predict')\n",
        "plt.hist(real_data, bins=160, color='red', alpha=0.5, label='last bought')\n",
        "\n",
        "plt.xlabel('voice_pkg_code')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('predict with linear regresion and ')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_next_pkg_linear(X_train,y_train,next_time):\n",
        "    encoder = OneHotEncoder()\n",
        "\n",
        "    # Fit the encoder to the 'voice_pkg_code' column\n",
        "    encoder.fit(y_train.reshape(-1, 1))\n",
        "\n",
        "    # Transform the 'voice_pkg_code' column into one-hot encoded features\n",
        "    encoded_features = encoder.transform(y_train.reshape(-1, 1)).toarray()\n",
        "\n",
        "    # Scale the X_train data\n",
        "    sc_X = StandardScaler()\n",
        "    X_train_scaled = sc_X.fit_transform(X_train)\n",
        "\n",
        "    lin_reg = LinearRegression()\n",
        "    lin_reg.fit(X_train_scaled, encoded_features)\n",
        "\n",
        "    # Perform decoding\n",
        "    predicted_features = lin_reg.predict(sc_X.transform(np.array([next_time])))\n",
        "    decoded_labels = encoder.inverse_transform(predicted_features)\n",
        "    return int(decoded_labels[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Function to predict the next package using Linear Regression\n",
        "def predict_next_pkg_svm_(X_train, y_train, next_time):\n",
        "    # Feature scaling using StandardScaler\n",
        "    sc_X = StandardScaler()\n",
        "    X_train_scaled = sc_X.fit_transform(X_train)\n",
        "    # print(y_train)\n",
        "    unique_values_count = len(set(y_train))\n",
        "    unique_values_count = len({x: True for x in y_train})\n",
        "\n",
        "    if(unique_values_count != 1):\n",
        "        classifier = svm.SVC( kernel = 'linear', decision_function_shape='ovr')  # You can choose different kernels like 'linear', 'rbf', 'poly', etc.\n",
        "        # Train the model\n",
        "        classifier.fit(X_train_scaled, y_train)\n",
        "        # Perform decoding\n",
        "        next_time_scaled = sc_X.transform(np.array([next_time]))\n",
        "        predicted_features = classifier.predict(next_time_scaled)\n",
        "        predicted_pkg =words_to_number(str(predicted_features[0]))\n",
        "        return predicted_pkg\n",
        "    else:\n",
        "        return words_to_number(y_train[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "range5to20_only_value(17)\n",
        "check_file_last_item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "real_data = []\n",
        "predict_nex_pkg_data_linear = []\n",
        "predict_nex_pkg_data_svm = []\n",
        "predict_nex_pkg_data_naive_bayes = []\n",
        "accurucy_ll = []\n",
        "least_num = []\n",
        "counter_cor=0\n",
        "counter_wrong = 0\n",
        "\n",
        "# Read each CSV file one by one\n",
        "# ,6\n",
        "columns_x =[4, 7, 23, 28]\n",
        "folder_name = 'unique_id_behavior'\n",
        "csv_files = [file for file in os.listdir(folder_name) if file.endswith('.csv')]\n",
        "for file in csv_files:\n",
        "       # Construct the file path\n",
        "       file_path = os.path.join(folder_name, file)\n",
        "       # Read the CSV file\n",
        "       df = pd.read_csv(file_path)\n",
        "       # Get information\n",
        "       y_train_word = []\n",
        "       X_train = df.iloc[:, columns_x].values\n",
        "       y_train = df[\"voice_pkg_code\"].values\n",
        "       X_train = X_train[:-1]\n",
        "       y_train = y_train[:-1]\n",
        "       sub_id = df[\"sub_id\"].iloc[-1]\n",
        "       last_pkg_bought = df[\"voice_pkg_code\"].iloc[-1]\n",
        "       next_time = df.iloc[-1, columns_x].values\n",
        "       real_data.append( df[\"voice_pkg_code\"].iloc[-1])\n",
        "       for i in range(len(y_train)):\n",
        "            y_train_word.append(number_to_words(y_train[i]))\n",
        "            \n",
        "       # Predict\n",
        "       y_pred_svm = predict_next_pkg_svm_(X_train,y_train_word , next_time)\n",
        "       y_pred_linear = predict_next_pkg_linear(X_train,y_train , next_time)\n",
        "       predict_nex_pkg_data_svm.append(y_pred_svm)\n",
        "       predict_nex_pkg_data_linear.append(y_pred_linear)\n",
        "       if last_pkg_bought == y_pred_svm:\n",
        "            print(sub_id ,'true pkg' , last_pkg_bought, \"SVM pred\",y_pred_svm , \"linear pred\",y_pred_linear )\n",
        "       if y_pred_linear == last_pkg_bought:\n",
        "            print(sub_id ,'true pkg' , last_pkg_bought, \"SVM pred\",y_pred_svm , \"linear pred\",y_pred_linear )\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
