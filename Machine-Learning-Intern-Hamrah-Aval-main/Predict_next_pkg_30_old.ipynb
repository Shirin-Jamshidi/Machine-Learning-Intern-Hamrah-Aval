{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lqdt-kjAE-vo"
      },
      "source": [
        "#All"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7Nz9gMBH9Dce"
      },
      "source": [
        "#####Import library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbkuQLeD8sUz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio\n",
        "import os\n",
        "from skimage import io\n",
        "from IPython.display import Image\n",
        "from PIL import Image\n",
        "from scipy.signal import correlate2d\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mHzTp1ps9OLF"
      },
      "source": [
        "#####Read file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vnljzYb9RlW",
        "outputId": "908a9a82-df78-4805-ec56-ddbbe3b4405b"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(r\"30.csv\")\n",
        "print(df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M-S8wpV599bt"
      },
      "source": [
        "#####find uniqe sub_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJzgB8yd_SCs",
        "outputId": "f89a3aad-56e1-4803-e150-79684d86ad28"
      },
      "outputs": [],
      "source": [
        "# Extract the unique IDs from the 'sub_id' column\n",
        "unique_ids = df['sub_id'].unique()\n",
        "\n",
        "# Print the unique IDs\n",
        "for unique_id in unique_ids:\n",
        "    print(unique_id)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Trjl7pMB_vRH"
      },
      "source": [
        "####how many uniqe id buy package and save data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B1aaWBYAMAKQ"
      },
      "source": [
        "#####count every user has how many pkg purchase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAZ_tIl0_u3M",
        "outputId": "632e2399-3f0d-4d10-8d5f-7f103a49355e"
      },
      "outputs": [],
      "source": [
        "# Count the occurrences of each unique ID\n",
        "id_counts = df['sub_id'].value_counts()\n",
        "\n",
        "# Print the ID counts\n",
        "for unique_id, count in id_counts.items():\n",
        "    print(f\"Unique ID: {unique_id}, Count: {count}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u1X1eWhKMXgh"
      },
      "source": [
        "##### plot data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "fqeXPm7-BP9D",
        "outputId": "5af6d8b1-9d91-4f52-a48a-bfb8afa2026f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "# Plotting the histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(id_counts, bins=100, alpha=0.75, color='blue')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Count of Unique IDs')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WfgFWCPFCo0x"
      },
      "source": [
        "##### create csv file for every unique id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl8fmrrNTKRd",
        "outputId": "0d5e6e19-682a-44ce-f9a9-91cd4900b4d6"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "#first delete\n",
        "\n",
        "\n",
        "folder_path = r'H:\\GIT project\\Hamrah Aval\\unique_id_behavior'\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(folder_path)\n",
        "    print(f\"Folder '{folder_path}' has been deleted successfully.\")\n",
        "except OSError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "#then create folder\n",
        "# Create the folder if it doesn't exist\n",
        "folder_name = 'unique_id_behavior'\n",
        "if not os.path.exists(folder_name):\n",
        "    os.makedirs(folder_name)\n",
        "\n",
        "# Create a dictionary to store lists for each unique ID\n",
        "id_lists = {unique_id: [] for unique_id in id_counts.index}\n",
        "\n",
        "# Iterate over each row and append to the corresponding list\n",
        "for index, row in df.iterrows():\n",
        "    sub_id = row['sub_id']\n",
        "    id_lists[sub_id].append(row.tolist())\n",
        "\n",
        "minimum_data_needed = 5\n",
        "# Save each list as a separate CSV file in the folder\n",
        "for unique_id, rows_list in id_lists.items():\n",
        "    if len(rows_list) >= minimum_data_needed:\n",
        "        # Create a DataFrame from the list of rows\n",
        "        sub_df = pd.DataFrame(rows_list, columns=df.columns)\n",
        "\n",
        "        # Sort the DataFrame based on the \"actvn_dt\" column\n",
        "        sub_df = sub_df.sort_values(by='actvn_dt')\n",
        "\n",
        "        # Add a new column \"times\" with incrementing values starting from 1\n",
        "        sub_df['times'] = range(1, len(sub_df) + 1)\n",
        "\n",
        "        # Save the DataFrame as a CSV file in the folder\n",
        "        file_path = os.path.join(folder_name, f'{unique_id}_30.csv')\n",
        "        sub_df.to_csv(file_path, index=False)\n",
        "        # print(f'Saved rows for Unique ID {unique_id} in {file_path}')\n",
        "    else:\n",
        "        print(f'Skipped saving rows for Unique ID {unique_id} (less than {minimum_data_needed} rows)')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xQTJzeBjNRRg"
      },
      "source": [
        "##### make it zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYDrjsOeEYbn",
        "outputId": "ea87d614-89da-4ea2-eb21-acdbc819beaa"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "# Create a zip file of the folder\n",
        "zip_file_name = 'unique_id_behavior.zip'\n",
        "with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, _, files in os.walk(folder_name):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path, os.path.basename(file_path))\n",
        "\n",
        "print(f'Successfully created {zip_file_name}.')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eco1tyQyNZzw"
      },
      "source": [
        "#### train and predict last bought"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### nom to word and inverse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from word2number import w2n\n",
        "import inflect\n",
        "\n",
        "def words_to_number(words):\n",
        "    try:\n",
        "        number = w2n.word_to_num(words)\n",
        "        return number\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "    \n",
        "print(words_to_number(\"thirty-nine\"))\n",
        "\n",
        "def number_to_words(number):\n",
        "    p = inflect.engine()\n",
        "    return p.number_to_words(number)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cDFPgh3Ax-Jh"
      },
      "source": [
        "##### predict next pkg linear reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfgMG8nz6Zuo"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# Function to predict the next package using Linear Regression\n",
        "def predict_next_pkg_linear(X_train, y_train, next_time):\n",
        "    # Feature scaling using StandardScaler\n",
        "    sc_X = StandardScaler()\n",
        "    X_train_scaled = sc_X.fit_transform(X_train)\n",
        "\n",
        "    # Encode the labels with OneHotEncoder\n",
        "    encoder = OneHotEncoder(sparse=False)\n",
        "    encoded_features = encoder.fit_transform(y_train)\n",
        "    # print(X_train_scaled)\n",
        "    # Fit Linear Regression model\n",
        "    lin_reg = LinearRegression()\n",
        "    lin_reg.fit(X_train_scaled, encoded_features)\n",
        "\n",
        "    # Perform decoding\n",
        "    next_time_scaled = sc_X.transform(np.array([next_time]))\n",
        "    predicted_features = lin_reg.predict(next_time_scaled)\n",
        "    \n",
        "    # Inverse transform the one-hot encoded matrix to get the predicted label\n",
        "    decoded_labels = encoder.inverse_transform(predicted_features)\n",
        "\n",
        "    return decoded_labels\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### predict next pkg SVM reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn import svm\n",
        "\n",
        "# Function to predict the next package using Linear Regression\n",
        "def predict_next_pkg_svm_(X_train, y_train, next_time):\n",
        "    # Feature scaling using StandardScaler\n",
        "    sc_X = StandardScaler()\n",
        "    X_train_scaled = sc_X.fit_transform(X_train)\n",
        "    # print(y_train)\n",
        "\n",
        "    unique_values_count = len(set(y_train))\n",
        "    unique_values_count = len({x: True for x in y_train})\n",
        "    if(unique_values_count != 1):\n",
        "        classifier = svm.SVC( kernel = 'linear', decision_function_shape='ovr')  # You can choose different kernels like 'linear', 'rbf', 'poly', etc.\n",
        "        # Train the model\n",
        "        classifier.fit(X_train_scaled, y_train)\n",
        "        # Perform decoding\n",
        "        next_time_scaled = sc_X.transform(np.array([next_time]))\n",
        "        predicted_features = classifier.predict(next_time_scaled)\n",
        "        predicted_pkg =words_to_number(str(predicted_features[0]))\n",
        "        \n",
        "        return predicted_pkg\n",
        "    else:\n",
        "        return words_to_number(y_train[-1])\n",
        "    # return decoded_labels\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### predict next with naive_bayes "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "# Function to predict the next package using Linear Regression\n",
        "def predict_next_pkg_naive_bayess(X_train, y_train, next_time):\n",
        "    # Feature scaling using StandardScaler\n",
        "    sc_X = StandardScaler()\n",
        "    X_train_scaled = sc_X.fit_transform(X_train)\n",
        "    # print(y_train)\n",
        "\n",
        "    unique_values_count = len(set(y_train))\n",
        "    unique_values_count = len({x: True for x in y_train})\n",
        "    if(unique_values_count != 1):\n",
        "        classifier = GaussianNB()\n",
        "        # Train the model\n",
        "        classifier.fit(X_train_scaled, y_train)\n",
        "        # Perform decoding\n",
        "        next_time_scaled = sc_X.transform(np.array([next_time]))\n",
        "        predicted_features = classifier.predict(next_time_scaled)\n",
        "        predicted_pkg =words_to_number(str(predicted_features[0]))\n",
        "        \n",
        "        return predicted_pkg\n",
        "    else:\n",
        "        return words_to_number(y_train[-1])\n",
        "    # return decoded_labels\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk7GaOYiNyXf"
      },
      "source": [
        "##### Read files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O8XOzPwmrXa"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "csv_files = [file for file in os.listdir(folder_name) if file.endswith('.csv')]\n",
        "real_data = []\n",
        "predict_nex_pkg_data_linear = []\n",
        "predict_nex_pkg_data_svm = []\n",
        "predict_nex_pkg_data_naive_bayes = []\n",
        "\n",
        "# Read each CSV file one by one\n",
        "columns_ = [5,24]\n",
        "\n",
        "for file in csv_files:\n",
        "    # Construct the file path\n",
        "    file_path = os.path.join(folder_name, file)\n",
        "\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Get information\n",
        "    X_train = df.iloc[:, columns_].values\n",
        "    # print(X_train)\n",
        "    y_train = df[\"voice_pkg_code\"].values.reshape(-1, 1)\n",
        "    df[['prod_stat_eng', 'gender', 'Pk_ATL/BTL', 'device_type', 'Pk_regular/Event']] = df[['prod_stat_eng', 'gender', 'Pk_ATL/BTL', 'device_type', 'Pk_regular/Event']].apply(LabelEncoder().fit_transform)\n",
        "    X_train = X_train[:-1]\n",
        "    y_train = np.delete(y_train, -1).tolist()\n",
        "    y_train_word = []\n",
        "    for i in range(len(y_train)):\n",
        "       y_train_word.append(number_to_words(y_train[i]))\n",
        "    sub_id = df[\"sub_id\"].iloc[-1]\n",
        "    next_time = df.iloc[-1, columns_].values\n",
        " \n",
        "    last_pkg_bought = df[\"voice_pkg_code\"].iloc[-1]\n",
        "    real_data.append( df[\"voice_pkg_code\"].iloc[-1])\n",
        "\n",
        "    # Predict\n",
        "    # y_pred_linear = predict_next_pkg_linear(X_train,y_train.reshape(-1, 1) , next_time)\n",
        "    # predict_nex_pkg_data_linear.append(int(y_pred_linear[0][0]))\n",
        "    y_pred_svm = predict_next_pkg_svm_(X_train,y_train_word , next_time)\n",
        "    predict_nex_pkg_data_svm.append(y_pred_svm)\n",
        "    y_pred_naive_bayes = predict_next_pkg_naive_bayess(X_train,y_train_word , next_time)\n",
        "    predict_nex_pkg_data_naive_bayes.append(y_pred_naive_bayes)\n",
        "    # if(last_pkg_bought != int(decoded_labels[0][0])):\n",
        "      # print(f'{sub_id}  last pkg bought : {last_pkg_bought}  next pkg predicted {int(decoded_labels[0][0])}.')\n",
        " "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkQ3_ISW7So5",
        "outputId": "3be170e0-8b8e-4d72-ccdb-abe86069074d"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score,precision_score, recall_score\n",
        "print(\"accuracy_score = \",accuracy_score(real_data,y_pred_naive_bayes))\n",
        "print(confusion_matrix(real_data, y_pred_naive_bayes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "7UCMVgSZsqA4",
        "outputId": "b5049f02-b7aa-461f-e312-28cd5cf060be"
      },
      "outputs": [],
      "source": [
        "# Plot the histograms\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.hist(predict_nex_pkg_data, bins=160, color='blue', alpha=0.5, label='predict')\n",
        "plt.hist(real_data, bins=160, color='red', alpha=0.5, label='last bought')\n",
        "\n",
        "plt.xlabel('voice_pkg_code')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('predict with linear regresion and ')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EaaDr7rj2ZjC"
      },
      "source": [
        "#### train and predict every user with times"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kG6O25Fb2ZjD"
      },
      "source": [
        "#####predict next pkg linear reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUYkrclB2ZjE"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "def predict_next_pkg_linear(X_train,y_train,next_time):\n",
        "    encoder = OneHotEncoder()\n",
        "\n",
        "    # Fit the encoder to the 'voice_pkg_code' column\n",
        "    encoder.fit(y_train.reshape(-1, 1))\n",
        "\n",
        "    # Transform the 'voice_pkg_code' column into one-hot encoded features\n",
        "    encoded_features = encoder.transform(y_train.reshape(-1, 1)).toarray()\n",
        "\n",
        "    # Scale the X_train data\n",
        "    sc_X = StandardScaler()\n",
        "    X_train_scaled = sc_X.fit_transform(X_train.reshape(-1, 1))\n",
        "\n",
        "    lin_reg = LinearRegression()\n",
        "    lin_reg.fit(X_train_scaled, encoded_features)\n",
        "\n",
        "    # Perform decoding\n",
        "    predicted_features = lin_reg.predict(sc_X.transform(np.array([next_time]).reshape(-1, 1)))\n",
        "    decoded_labels = encoder.inverse_transform(predicted_features)\n",
        "    return decoded_labels\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eNgUnlxt2ZjE"
      },
      "source": [
        "#####Read files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTRck9VI2ZjE",
        "outputId": "63bc3bc8-5330-4f5c-cef1-0fe2b0ff2298"
      },
      "outputs": [],
      "source": [
        "# Get a list of all CSV files in the folder\n",
        "csv_files = [file for file in os.listdir(folder_name) if file.endswith('.csv')]\n",
        "real_data = []\n",
        "predict_nex_pkg_data = []\n",
        "# Read each CSV file one by one\n",
        "for file in csv_files:\n",
        "    # Construct the file path\n",
        "    file_path = os.path.join(folder_name, file)\n",
        "\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Get information\n",
        "    X_train = np.ravel(np.array(df[\"times\"].values).reshape(-1, 1))\n",
        "    # X_train = np.ravel(np.array(df[\"actvn_dt\"].values).reshape(-1, 1))\n",
        "    y_train = np.ravel(np.array(df[\"voice_pkg_code\"].values).reshape(-1, 1))\n",
        "    sub_id = df[\"sub_id\"].iloc[-1]\n",
        "    next_time = df[\"times\"].iloc[-1] + 1\n",
        "    last_pkg_bought = df[\"voice_pkg_code\"].iloc[-1]\n",
        "    real_data.append( df[\"voice_pkg_code\"].iloc[-1])\n",
        "\n",
        "    # Predict\n",
        "    decoded_labels = predict_next_pkg_linear(X_train,y_train , next_time)\n",
        "    if(last_pkg_bought != int(decoded_labels[0][0])):\n",
        "      print(f'{sub_id}  last pkg bought : {last_pkg_bought}  next pkg predicted {decoded_labels}.')\n",
        "    predict_nex_pkg_data.append(int(decoded_labels[0][0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "1AevCqPY2ZjF",
        "outputId": "a20563d3-8abf-41a8-a017-7c871f9f4a64"
      },
      "outputs": [],
      "source": [
        "# Plot the histograms\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.hist(predict_nex_pkg_data, bins=160, color='blue', alpha=0.5, label='predict')\n",
        "plt.hist(real_data, bins=160, color='red', alpha=0.5, label='last bought')\n",
        "\n",
        "plt.xlabel('voice_pkg_code')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('predict with linear regresion and ')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
